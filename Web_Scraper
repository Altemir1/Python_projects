import requests
from bs4 import BeautifulSoup
import os
import string

page_number = int(input())
article_type = input()
parent = os.getcwd()

for i in range(1, page_number + 1):
    url = f'https://www.nature.com/nature/articles?sort=PubDate&year=2020&page={str(i)}'

    response = requests.get(url)

    if response:
        if not os.path.exists(os.path.join(parent, f'Page_{str(i)}')):
            os.mkdir(f'Page_{str(i)}')

        soup = BeautifulSoup(response.content, "html.parser")

        articles = soup.find_all('article')

        for article in articles:

            found_article_types = article.find('span', {'data-test': 'article.type'}).text.strip()

            if found_article_types == article_type:

                article_link = article.find('a', {'data-track-action': 'view article'})['href']


                article_response = requests.get('https://www.nature.com' + article_link)

                if article_response.status_code:

                    article_soup = BeautifulSoup(article_response.content, 'html.parser')


                    article_title = article_soup.find('h1', {'class': 'c-article-magazine-title'}).text.strip()

                    article_title = article_title.replace(" ", "_")

                    article_body = article_soup.find('p', {'class': 'article__teaser'}).text.strip()

                    filename = os.path.join(f'Page_{str(i)}', f'{article_title}.txt')

                    with open(filename, 'w', encoding='utf-8') as file:
                        file.write(article_body)
    else:
        print("Page not found")
